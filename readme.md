# ğŸ§ª LLM Prompt Evaluation Lab

This is my first hands-on project to benchmark local LLMs using zero-shot and one-shot prompts across a variety of real-world tasks.

I scored 31 different models on a 0â€“5 scale in categories like:

- ğŸ§  Logic
- ğŸ§® Math
- ğŸ‘¨â€ğŸ’» Coding
- ğŸ¨ Creativity
- ğŸ©º Medical terminology
- ğŸ¤– Ambiguity handling
- ğŸ› ï¸ Error resolution

## ğŸ“ Contents

- `LLM_analysis.ipynb` â€“ My Jupyter notebook using pandas + matplotlib
- `LLM-zero-one-shot.csv` â€“ Raw scoring results
- `llm-accuracy-chart.png` â€“ (Optional) output chart

## ğŸ“– Related Blog Post

I wrote about this experiment and what I learned:
ğŸ‘‰ [Read the blog post](https://chicanoinparis.github.io/Prompt-Engineer-Portfolio/2025/06/19/my-first-dive.html)

## ğŸ§ª Try It Yourself

You can preview the notebook in your browser using nbviewer:

ğŸ‘‰ [View notebook on nbviewer](https://nbviewer.org/github/ChicanoInParis/LLM-Prompt-Evauation-Lab/blob/main/LLM_analysis.ipynb)

---

![LLM Accuracy Chart](Average%20Accuracy%20Score.png)

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange?logo=jupyter)
![License](https://img.shields.io/badge/License-MIT-green?logo=open-source-initiative)
![Status](https://img.shields.io/badge/Stage-First%20Round-lightgrey)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ChicanoInParis/LLM-Prompt-Evaluation-Lab/blob/main/LLM_Prompt_Eval.ipynb)

[ğŸ” View on nbviewer](https://nbviewer.org/github/ChicanoInParis/LLM-Prompt-Evaluation-Lab/blob/main/LLM_Prompt_Eval.ipynb)


